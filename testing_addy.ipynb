{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import graphviz\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112, 11)\n",
      "(1129, 27)\n"
     ]
    }
   ],
   "source": [
    "full_results = pd.read_csv('full_results.csv')\n",
    "full_stats = pd.read_csv('FullKenPom_pt.csv')\n",
    "\n",
    "print(full_results.shape)\n",
    "print(full_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Round</th>\n",
       "      <th>Region Number</th>\n",
       "      <th>Region Name</th>\n",
       "      <th>SeedA</th>\n",
       "      <th>ScoreA</th>\n",
       "      <th>TeamA</th>\n",
       "      <th>TeamB</th>\n",
       "      <th>ScoreB</th>\n",
       "      <th>SeedB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>Duke</td>\n",
       "      <td>Winthrop</td>\n",
       "      <td>37</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Florida Atlantic</td>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>3</td>\n",
       "      <td>71</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Central Connecticut St</td>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>USC</td>\n",
       "      <td>UNC Wilmington</td>\n",
       "      <td>93</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Utah</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  Unnamed: 0  Year  Round  Region Number Region Name  SeedA  \\\n",
       "0        1      1           1  2002      1              1       South      1   \n",
       "1        2      2           2  2002      1              1       South      2   \n",
       "2        3      3           3  2002      1              1       South      3   \n",
       "3        4      4           4  2002      1              1       South      4   \n",
       "4        5      5           5  2002      1              1       South      5   \n",
       "\n",
       "   ScoreA       TeamA                   TeamB  ScoreB  SeedB  \n",
       "0      84        Duke                Winthrop      37     16  \n",
       "1      86     Alabama        Florida Atlantic      78     15  \n",
       "2      71  Pittsburgh  Central Connecticut St      54     14  \n",
       "3      89         USC          UNC Wilmington      93     13  \n",
       "4      75     Indiana                    Utah      56     12  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###mask test and train data###\n",
    "res_test_mask = (full_results['Year'] == 2016) | (full_results['Year'] == 2017) | (full_results['Year'] == 2018)\n",
    "stats_test_mask = (full_stats['Season'] == 2016) | (full_stats['Season'] == 2017) | (full_stats['Season'] == 2018)\n",
    "res_train_mask = (full_results['Year'] < 2016)\n",
    "stats_train_mask = (full_stats['Season'] < 2016)\n",
    "\n",
    "res_data_test = full_results[res_test_mask]\n",
    "stats_data_test = full_stats[stats_test_mask]\n",
    "res_data_train = full_results[res_train_mask]\n",
    "stats_data_train = full_stats[stats_train_mask]\n",
    "\n",
    "#reindex all of them\n",
    "res_data_test = res_data_test.reset_index()\n",
    "stats_data_test = stats_data_test.reset_index()\n",
    "res_data_train = res_data_train.reset_index()\n",
    "stats_data_train = stats_data_train.reset_index()\n",
    "\n",
    "for i in range(len(res_data_test)):\n",
    "    if res_data_test['Region Name'][i] == \"First Four\":\n",
    "        res_data_test = res_data_test.drop(i)\n",
    "for i in range(len(res_data_train)):\n",
    "    if res_data_train['Region Name'][i] == \"First Four\":\n",
    "        res_data_train = res_data_train.drop(i)\n",
    "        \n",
    "#reindex all of them\n",
    "res_data_test = res_data_test.reset_index()\n",
    "stats_data_test = stats_data_test.reset_index()\n",
    "res_data_train = res_data_train.reset_index()\n",
    "stats_data_train = stats_data_train.reset_index()\n",
    "\n",
    "res_data_train.head()\n",
    "\n",
    "#res_data_test.tail()\n",
    "#stats_data_test.tail()\n",
    "#res_data_train.tail()\n",
    "#stats_data_train.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "#these names match the kenpom stats csv\n",
    "stats_vec = [\"AdjTempo\",\n",
    "            \"AdjOE\",\n",
    "            \"AdjDE\",\n",
    "            \"AdjEM\",\n",
    "            \"seed\",\n",
    "            \"ConfTournament\",\n",
    "            \"SOSAdjEM\",\n",
    "            \"NCSOSAdjEM\",\n",
    "            \"O-D_eFG_Pct\",\n",
    "            \"D-O_TO_Pct\",\n",
    "            \"O-D_OR_Pct\",\n",
    "            \"O-D_FT_Rate\",\n",
    "            \"LastTenRecord\"]\n",
    "\n",
    "print(len(stats_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move training data into 2 numpy arrays - data and labels (results)\n",
    "N = len(res_data_train)\n",
    "\n",
    "training_data = np.zeros((N,13))\n",
    "training_labels = np.zeros((N,1))\n",
    "\n",
    "for i in range(len(res_data_train)):\n",
    "    year = res_data_train['Year'][i]\n",
    "    teamA = res_data_train['TeamA'][i]\n",
    "    teamB = res_data_train['TeamB'][i]\n",
    "    score_diff = res_data_train['ScoreA'][i] - res_data_train['ScoreB'][i]\n",
    "    for k in range(len(stats_data_train)):\n",
    "        if ((stats_data_train['Season'][k] == year) and (stats_data_train['TeamName'][k] == teamA)):\n",
    "            indexA = k\n",
    "            break\n",
    "            \n",
    "    for k in range(len(stats_data_train)):\n",
    "        if ((stats_data_train['Season'][k] == year) and (stats_data_train['TeamName'][k] == teamB)):\n",
    "            indexB = k\n",
    "            break\n",
    "    for s in range(len(stats_vec)):\n",
    "        stat = stats_vec[s]\n",
    "        training_data[i][s] = stats_data_train[stat][indexA] - stats_data_train[stat][indexB]\n",
    "    \n",
    "    if (score_diff > 0): # team A won\n",
    "        training_labels[i][0] = 1\n",
    "    else:\n",
    "        training_labels[i][0] = 0 #team B won\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move testing data into 2 numpy arrays - data and labels (results)\n",
    "N = len(res_data_test)\n",
    "\n",
    "testing_data = np.zeros((N,13))\n",
    "testing_labels = np.zeros((N,1))\n",
    "\n",
    "for i in range(len(res_data_test)):\n",
    "#for i in range(5):\n",
    "    year = res_data_test['Year'][i]\n",
    "    teamA = res_data_test['TeamA'][i]\n",
    "    teamB = res_data_test['TeamB'][i]\n",
    "    score_diff = res_data_test['ScoreA'][i] - res_data_test['ScoreB'][i]\n",
    "    for k in range(len(stats_data_test)):\n",
    "        if ((stats_data_test['Season'][k] == year) and (stats_data_test['TeamName'][k] == teamA)):\n",
    "            indexA = k\n",
    "            break\n",
    "            \n",
    "    for k in range(len(stats_data_test)):\n",
    "        if ((stats_data_test['Season'][k] == year) and (stats_data_test['TeamName'][k] == teamB)):\n",
    "            indexB = k\n",
    "            break\n",
    "    for s in range(len(stats_vec)):\n",
    "        stat = stats_vec[s]\n",
    "        testing_data[i][s] = stats_data_test[stat][indexA] - stats_data_test[stat][indexB]\n",
    "    \n",
    "    if (score_diff > 0):\n",
    "        testing_labels[i][0] = 1\n",
    "    else:\n",
    "        testing_labels[i][0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(882, 13)\n",
      "(882, 1)\n",
      "\n",
      "\n",
      "(189, 13)\n",
      "(189, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "print(training_labels.shape)\n",
    "print('\\n')\n",
    "print(testing_data.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Time to normalize the data\n",
    "training_data = preprocessing.normalize(training_data, axis=0, norm='max')\n",
    "testing_data = preprocessing.normalize(testing_data, axis=0, norm='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(training_data, label=training_labels,\n",
    "                     feature_names=stats_vec)\n",
    "dtest = xgb.DMatrix(testing_data, label=testing_labels,\n",
    "                    feature_names=stats_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=1, min_child_weight=1\n",
      "\tMerror 0.2834466666666667 for 0 rounds\n",
      "CV with max_depth=1, min_child_weight=2\n",
      "\tMerror 0.2834466666666667 for 0 rounds\n",
      "CV with max_depth=1, min_child_weight=3\n",
      "\tMerror 0.2834466666666667 for 0 rounds\n",
      "CV with max_depth=1, min_child_weight=4\n",
      "\tMerror 0.2834466666666667 for 0 rounds\n",
      "CV with max_depth=1, min_child_weight=5\n",
      "\tMerror 0.2834466666666667 for 0 rounds\n",
      "CV with max_depth=2, min_child_weight=1\n",
      "\tMerror 0.27210866666666667 for 23 rounds\n",
      "CV with max_depth=2, min_child_weight=2\n",
      "\tMerror 0.2732426666666667 for 20 rounds\n",
      "CV with max_depth=2, min_child_weight=3\n",
      "\tMerror 0.26870733333333335 for 14 rounds\n",
      "CV with max_depth=2, min_child_weight=4\n",
      "\tMerror 0.26757366666666665 for 18 rounds\n",
      "CV with max_depth=2, min_child_weight=5\n",
      "\tMerror 0.2732426666666667 for 5 rounds\n",
      "CV with max_depth=3, min_child_weight=1\n",
      "\tMerror 0.27097533333333335 for 2 rounds\n",
      "CV with max_depth=3, min_child_weight=2\n",
      "\tMerror 0.26870733333333335 for 2 rounds\n",
      "CV with max_depth=3, min_child_weight=3\n",
      "\tMerror 0.2698413333333333 for 2 rounds\n",
      "CV with max_depth=3, min_child_weight=4\n",
      "\tMerror 0.2687076666666666 for 2 rounds\n",
      "CV with max_depth=3, min_child_weight=5\n",
      "\tMerror 0.27664399999999995 for 2 rounds\n",
      "CV with max_depth=4, min_child_weight=1\n",
      "\tMerror 0.282313 for 2 rounds\n",
      "CV with max_depth=4, min_child_weight=2\n",
      "\tMerror 0.285714 for 4 rounds\n",
      "CV with max_depth=4, min_child_weight=3\n",
      "\tMerror 0.29251699999999997 for 4 rounds\n",
      "CV with max_depth=4, min_child_weight=4\n",
      "\tMerror 0.285714 for 7 rounds\n",
      "CV with max_depth=4, min_child_weight=5\n",
      "\tMerror 0.283447 for 11 rounds\n",
      "CV with max_depth=5, min_child_weight=1\n",
      "\tMerror 0.2868483333333333 for 5 rounds\n",
      "CV with max_depth=5, min_child_weight=2\n",
      "\tMerror 0.28571399999999997 for 26 rounds\n",
      "CV with max_depth=5, min_child_weight=3\n",
      "\tMerror 0.281179 for 6 rounds\n",
      "CV with max_depth=5, min_child_weight=4\n",
      "\tMerror 0.29024933333333336 for 9 rounds\n",
      "CV with max_depth=5, min_child_weight=5\n",
      "\tMerror 0.28458066666666665 for 11 rounds\n",
      "CV with max_depth=6, min_child_weight=1\n",
      "\tMerror 0.2834466666666667 for 19 rounds\n",
      "CV with max_depth=6, min_child_weight=2\n",
      "\tMerror 0.281179 for 10 rounds\n",
      "CV with max_depth=6, min_child_weight=3\n",
      "\tMerror 0.2732426666666667 for 6 rounds\n",
      "CV with max_depth=6, min_child_weight=4\n",
      "\tMerror 0.28911566666666666 for 23 rounds\n",
      "CV with max_depth=6, min_child_weight=5\n",
      "\tMerror 0.27664399999999995 for 4 rounds\n",
      "CV with max_depth=7, min_child_weight=1\n",
      "\tMerror 0.2913833333333333 for 13 rounds\n",
      "CV with max_depth=7, min_child_weight=2\n",
      "\tMerror 0.2834466666666667 for 6 rounds\n",
      "CV with max_depth=7, min_child_weight=3\n",
      "\tMerror 0.2845803333333334 for 6 rounds\n",
      "CV with max_depth=7, min_child_weight=4\n",
      "\tMerror 0.2811793333333334 for 8 rounds\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "\tMerror 0.28571433333333335 for 7 rounds\n"
     ]
    }
   ],
   "source": [
    "#Phase 1: Tuning Max depth and min_child_weight\n",
    "param = {'objective': 'multi:softprob'}\n",
    "param['eval_metric'] = \"merror\"\n",
    "param['num_class'] = 2  # 2 classes - win or loss\n",
    "\n",
    "#evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 999 #looks like it levels off at around 200\n",
    "\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(1,8)\n",
    "    for min_child_weight in range(1,6)\n",
    "]\n",
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    \n",
    "    # Update Parameters\n",
    "    param['max_depth'] = max_depth\n",
    "    param['min_child_weight'] = min_child_weight\n",
    "    \n",
    "    #Run CV\n",
    "    cv_results = xgb.cv(param,\n",
    "                        dtrain,\n",
    "                        num_boost_round=num_round, #maybe wrong\n",
    "                        seed=42,\n",
    "                        nfold=3,\n",
    "                        metrics={'merror'},\n",
    "                        early_stopping_rounds=10)\n",
    "    \n",
    "    #Update best MError\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_merror\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "    \n",
    "\n",
    "param['max_depth'] = best_params[0]\n",
    "param['min_child_weight'] = best_params[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "\tMerror 0.26757366666666665 for 18 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "\tMerror 0.269841 for 16 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tMerror 0.2619046666666667 for 22 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "\tMerror 0.2721086666666666 for 19 rounds\n",
      "CV with subsample=1.0, colsample=0.6\n",
      "\tMerror 0.27437633333333333 for 6 rounds\n",
      "CV with subsample=1.0, colsample=0.5\n",
      "\tMerror 0.2743766666666667 for 1 rounds\n",
      "CV with subsample=1.0, colsample=0.4\n",
      "\tMerror 0.27437633333333333 for 12 rounds\n",
      "CV with subsample=1.0, colsample=0.3\n",
      "\tMerror 0.2766436666666667 for 6 rounds\n",
      "CV with subsample=1.0, colsample=0.2\n",
      "\tMerror 0.281179 for 1 rounds\n",
      "CV with subsample=1.0, colsample=0.1\n",
      "\tMerror 0.28231300000000004 for 7 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tMerror 0.27324233333333336 for 1 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tMerror 0.270975 for 10 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tMerror 0.25963733333333333 for 16 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tMerror 0.2732426666666667 for 8 rounds\n",
      "CV with subsample=0.9, colsample=0.6\n",
      "\tMerror 0.27210866666666667 for 12 rounds\n",
      "CV with subsample=0.9, colsample=0.5\n",
      "\tMerror 0.27097499999999997 for 13 rounds\n",
      "CV with subsample=0.9, colsample=0.4\n",
      "\tMerror 0.277778 for 6 rounds\n",
      "CV with subsample=0.9, colsample=0.3\n",
      "\tMerror 0.2800453333333333 for 6 rounds\n",
      "CV with subsample=0.9, colsample=0.2\n",
      "\tMerror 0.282313 for 0 rounds\n",
      "CV with subsample=0.9, colsample=0.1\n",
      "\tMerror 0.282313 for 7 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tMerror 0.26643966666666663 for 13 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tMerror 0.26870733333333335 for 10 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tMerror 0.2800453333333333 for 20 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tMerror 0.28231266666666666 for 16 rounds\n",
      "CV with subsample=0.8, colsample=0.6\n",
      "\tMerror 0.27097499999999997 for 13 rounds\n",
      "CV with subsample=0.8, colsample=0.5\n",
      "\tMerror 0.2811793333333333 for 1 rounds\n",
      "CV with subsample=0.8, colsample=0.4\n",
      "\tMerror 0.27777766666666664 for 3 rounds\n",
      "CV with subsample=0.8, colsample=0.3\n",
      "\tMerror 0.2800453333333333 for 5 rounds\n",
      "CV with subsample=0.8, colsample=0.2\n",
      "\tMerror 0.2834466666666667 for 0 rounds\n",
      "CV with subsample=0.8, colsample=0.1\n",
      "\tMerror 0.282313 for 3 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "\tMerror 0.28684766666666667 for 2 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tMerror 0.255102 for 5 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tMerror 0.26303866666666664 for 7 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tMerror 0.2721086666666666 for 7 rounds\n",
      "CV with subsample=0.7, colsample=0.6\n",
      "\tMerror 0.27097499999999997 for 9 rounds\n",
      "CV with subsample=0.7, colsample=0.5\n",
      "\tMerror 0.2800453333333333 for 7 rounds\n",
      "CV with subsample=0.7, colsample=0.4\n",
      "\tMerror 0.27891166666666667 for 1 rounds\n",
      "CV with subsample=0.7, colsample=0.3\n",
      "\tMerror 0.27437666666666666 for 8 rounds\n",
      "CV with subsample=0.7, colsample=0.2\n",
      "\tMerror 0.27777766666666664 for 11 rounds\n",
      "CV with subsample=0.7, colsample=0.1\n",
      "\tMerror 0.2811793333333333 for 3 rounds\n",
      "CV with subsample=0.6, colsample=1.0\n",
      "\tMerror 0.27324299999999996 for 8 rounds\n",
      "CV with subsample=0.6, colsample=0.9\n",
      "\tMerror 0.258503 for 5 rounds\n",
      "CV with subsample=0.6, colsample=0.8\n",
      "\tMerror 0.258503 for 5 rounds\n",
      "CV with subsample=0.6, colsample=0.7\n",
      "\tMerror 0.2641723333333334 for 6 rounds\n",
      "CV with subsample=0.6, colsample=0.6\n",
      "\tMerror 0.26077100000000003 for 5 rounds\n",
      "CV with subsample=0.6, colsample=0.5\n",
      "\tMerror 0.26870733333333335 for 1 rounds\n",
      "CV with subsample=0.6, colsample=0.4\n",
      "\tMerror 0.27777766666666664 for 9 rounds\n",
      "CV with subsample=0.6, colsample=0.3\n",
      "\tMerror 0.27664399999999995 for 7 rounds\n",
      "CV with subsample=0.6, colsample=0.2\n",
      "\tMerror 0.2755103333333333 for 9 rounds\n",
      "CV with subsample=0.6, colsample=0.1\n",
      "\tMerror 0.282313 for 2 rounds\n",
      "CV with subsample=0.5, colsample=1.0\n",
      "\tMerror 0.2562356666666667 for 25 rounds\n",
      "CV with subsample=0.5, colsample=0.9\n",
      "\tMerror 0.2562356666666667 for 25 rounds\n",
      "CV with subsample=0.5, colsample=0.8\n",
      "\tMerror 0.2619046666666667 for 24 rounds\n",
      "CV with subsample=0.5, colsample=0.7\n",
      "\tMerror 0.264172 for 11 rounds\n",
      "CV with subsample=0.5, colsample=0.6\n",
      "\tMerror 0.2721086666666666 for 25 rounds\n",
      "CV with subsample=0.5, colsample=0.5\n",
      "\tMerror 0.27097499999999997 for 9 rounds\n",
      "CV with subsample=0.5, colsample=0.4\n",
      "\tMerror 0.2766436666666667 for 6 rounds\n",
      "CV with subsample=0.5, colsample=0.3\n",
      "\tMerror 0.276644 for 6 rounds\n",
      "CV with subsample=0.5, colsample=0.2\n",
      "\tMerror 0.277778 for 6 rounds\n",
      "CV with subsample=0.5, colsample=0.1\n",
      "\tMerror 0.2811793333333334 for 9 rounds\n",
      "CV with subsample=0.4, colsample=1.0\n",
      "\tMerror 0.265306 for 14 rounds\n",
      "CV with subsample=0.4, colsample=0.9\n",
      "\tMerror 0.264172 for 11 rounds\n",
      "CV with subsample=0.4, colsample=0.8\n",
      "\tMerror 0.25963699999999995 for 11 rounds\n",
      "CV with subsample=0.4, colsample=0.7\n",
      "\tMerror 0.2630383333333333 for 8 rounds\n",
      "CV with subsample=0.4, colsample=0.6\n",
      "\tMerror 0.2721086666666666 for 0 rounds\n",
      "CV with subsample=0.4, colsample=0.5\n",
      "\tMerror 0.27097466666666664 for 0 rounds\n",
      "CV with subsample=0.4, colsample=0.4\n",
      "\tMerror 0.28231266666666666 for 3 rounds\n",
      "CV with subsample=0.4, colsample=0.3\n",
      "\tMerror 0.2732426666666667 for 14 rounds\n",
      "CV with subsample=0.4, colsample=0.2\n",
      "\tMerror 0.281179 for 4 rounds\n",
      "CV with subsample=0.4, colsample=0.1\n",
      "\tMerror 0.28231266666666666 for 14 rounds\n",
      "CV with subsample=0.3, colsample=1.0\n",
      "\tMerror 0.26077100000000003 for 0 rounds\n",
      "CV with subsample=0.3, colsample=0.9\n",
      "\tMerror 0.26757366666666665 for 4 rounds\n",
      "CV with subsample=0.3, colsample=0.8\n",
      "\tMerror 0.25963699999999995 for 5 rounds\n",
      "CV with subsample=0.3, colsample=0.7\n",
      "\tMerror 0.2641723333333334 for 9 rounds\n",
      "CV with subsample=0.3, colsample=0.6\n",
      "\tMerror 0.265306 for 8 rounds\n",
      "CV with subsample=0.3, colsample=0.5\n",
      "\tMerror 0.2732426666666667 for 6 rounds\n",
      "CV with subsample=0.3, colsample=0.4\n",
      "\tMerror 0.27551 for 8 rounds\n",
      "CV with subsample=0.3, colsample=0.3\n",
      "\tMerror 0.27891166666666667 for 6 rounds\n",
      "CV with subsample=0.3, colsample=0.2\n",
      "\tMerror 0.28004566666666664 for 5 rounds\n",
      "CV with subsample=0.3, colsample=0.1\n",
      "\tMerror 0.27891166666666667 for 10 rounds\n",
      "CV with subsample=0.2, colsample=1.0\n",
      "\tMerror 0.27097499999999997 for 14 rounds\n",
      "CV with subsample=0.2, colsample=0.9\n",
      "\tMerror 0.2732426666666667 for 8 rounds\n",
      "CV with subsample=0.2, colsample=0.8\n",
      "\tMerror 0.2732426666666667 for 8 rounds\n",
      "CV with subsample=0.2, colsample=0.7\n",
      "\tMerror 0.2732426666666667 for 8 rounds\n",
      "CV with subsample=0.2, colsample=0.6\n",
      "\tMerror 0.2687076666666666 for 7 rounds\n",
      "CV with subsample=0.2, colsample=0.5\n",
      "\tMerror 0.2698413333333333 for 8 rounds\n",
      "CV with subsample=0.2, colsample=0.4\n",
      "\tMerror 0.26870733333333335 for 9 rounds\n",
      "CV with subsample=0.2, colsample=0.3\n",
      "\tMerror 0.2687076666666666 for 10 rounds\n",
      "CV with subsample=0.2, colsample=0.2\n",
      "\tMerror 0.2732426666666667 for 4 rounds\n",
      "CV with subsample=0.2, colsample=0.1\n",
      "\tMerror 0.27551033333333336 for 9 rounds\n",
      "CV with subsample=0.1, colsample=1.0\n",
      "\tMerror 0.27551 for 12 rounds\n",
      "CV with subsample=0.1, colsample=0.9\n",
      "\tMerror 0.2766436666666667 for 21 rounds\n",
      "CV with subsample=0.1, colsample=0.8\n",
      "\tMerror 0.2641723333333333 for 22 rounds\n",
      "CV with subsample=0.1, colsample=0.7\n",
      "\tMerror 0.27097499999999997 for 22 rounds\n",
      "CV with subsample=0.1, colsample=0.6\n",
      "\tMerror 0.2755103333333333 for 19 rounds\n",
      "CV with subsample=0.1, colsample=0.5\n",
      "\tMerror 0.26757366666666665 for 29 rounds\n",
      "CV with subsample=0.1, colsample=0.4\n",
      "\tMerror 0.29251699999999997 for 16 rounds\n",
      "CV with subsample=0.1, colsample=0.3\n",
      "\tMerror 0.28231300000000004 for 3 rounds\n",
      "CV with subsample=0.1, colsample=0.2\n",
      "\tMerror 0.282313 for 6 rounds\n",
      "CV with subsample=0.1, colsample=0.1\n",
      "\tMerror 0.27437633333333333 for 6 rounds\n"
     ]
    }
   ],
   "source": [
    "#Phase 2: Subsample and Colsample_bytree\n",
    "#tune subsample,colsample\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(1,11)]\n",
    "    for colsample in [i/10. for i in range(1,11)]\n",
    "]\n",
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "    # Update our parameters\n",
    "    param['subsample'] = subsample\n",
    "    param['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        seed=42,\n",
    "        nfold=3,\n",
    "        metrics={'merror'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best Merror\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_merror\n",
    "        best_params = (subsample,colsample)\n",
    "        \n",
    "        \n",
    "param['subsample'] = best_params[0]\n",
    "param['colsample_bytree'] = best_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.5\n",
      "\tMerror 0.2619046666666667 for 8 rounds\n",
      "CV with eta=0.3\n",
      "\tMerror 0.255102 for 5 rounds\n",
      "CV with eta=0.03\n",
      "\tMerror 0.267574 for 16 rounds\n",
      "CV with eta=0.003\n",
      "\tMerror 0.2687076666666666 for 2 rounds\n",
      "CV with eta=0.0003\n",
      "\tMerror 0.2687076666666666 for 2 rounds\n"
     ]
    }
   ],
   "source": [
    "#Phase 3: eta\n",
    "min_merror = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [0.5,0.3, 0.03, .003,0.0003]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # Update our parameters\n",
    "    param['eta'] = eta\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        seed=42,\n",
    "        nfold=3,\n",
    "        metrics={'merror'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best Merror\n",
    "    mean_merror = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\tMerror {} for {} rounds\".format(mean_merror, boost_rounds))\n",
    "    if mean_merror < min_merror:\n",
    "        min_merror = mean_merror\n",
    "        best_params = eta\n",
    "        \n",
    "param['eta'] = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-merror:0.312169\n",
      "Will train until Test-merror hasn't improved in 10 rounds.\n",
      "[1]\tTest-merror:0.275132\n",
      "[2]\tTest-merror:0.275132\n",
      "[3]\tTest-merror:0.285714\n",
      "[4]\tTest-merror:0.269841\n",
      "[5]\tTest-merror:0.259259\n",
      "[6]\tTest-merror:0.269841\n",
      "[7]\tTest-merror:0.259259\n",
      "[8]\tTest-merror:0.26455\n",
      "[9]\tTest-merror:0.269841\n",
      "[10]\tTest-merror:0.285714\n",
      "[11]\tTest-merror:0.275132\n",
      "[12]\tTest-merror:0.296296\n",
      "[13]\tTest-merror:0.275132\n",
      "[14]\tTest-merror:0.285714\n",
      "[15]\tTest-merror:0.269841\n",
      "Stopping. Best iteration:\n",
      "[5]\tTest-merror:0.259259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_gb=xgb.train(param,dtrain,num_boost_round=num_round,\n",
    "                   early_stopping_rounds=10,evals=[(dtest, \"Test\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12604ef28>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAEWCAYAAAANV2yLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12621bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(final_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.027634  , 0.972366  ],\n",
       "       [0.0257523 , 0.9742477 ],\n",
       "       [0.04518213, 0.9548179 ],\n",
       "       [0.14017299, 0.859827  ],\n",
       "       [0.13375519, 0.8662448 ],\n",
       "       [0.41473705, 0.58526295],\n",
       "       [0.18309265, 0.8169073 ],\n",
       "       [0.76848763, 0.23151243],\n",
       "       [0.02488676, 0.9751133 ],\n",
       "       [0.08540676, 0.9145932 ],\n",
       "       [0.05099088, 0.9490092 ],\n",
       "       [0.11050506, 0.88949496],\n",
       "       [0.21209793, 0.78790206],\n",
       "       [0.27049297, 0.729507  ],\n",
       "       [0.5461494 , 0.45385063],\n",
       "       [0.5789856 , 0.42101446],\n",
       "       [0.02756447, 0.97243553],\n",
       "       [0.04099805, 0.95900196],\n",
       "       [0.0697208 , 0.93027914],\n",
       "       [0.05238493, 0.9476151 ],\n",
       "       [0.14622635, 0.85377365],\n",
       "       [0.33794752, 0.66205245],\n",
       "       [0.39548248, 0.6045175 ],\n",
       "       [0.59507895, 0.40492108],\n",
       "       [0.03107248, 0.9689275 ],\n",
       "       [0.11638568, 0.88361436],\n",
       "       [0.1976761 , 0.80232394],\n",
       "       [0.06473161, 0.93526834],\n",
       "       [0.21901304, 0.7809869 ],\n",
       "       [0.34959218, 0.65040785],\n",
       "       [0.5892834 , 0.41071665],\n",
       "       [0.48420978, 0.5157902 ],\n",
       "       [0.10045514, 0.8995449 ],\n",
       "       [0.21839553, 0.78160447],\n",
       "       [0.34121612, 0.6587839 ],\n",
       "       [0.1663513 , 0.8336487 ],\n",
       "       [0.16175677, 0.83824325],\n",
       "       [0.22596475, 0.7740353 ],\n",
       "       [0.14678171, 0.8532183 ],\n",
       "       [0.09312334, 0.9068766 ],\n",
       "       [0.1192528 , 0.8807472 ],\n",
       "       [0.41185278, 0.58814716],\n",
       "       [0.35141614, 0.6485839 ],\n",
       "       [0.31668976, 0.6833102 ],\n",
       "       [0.12383259, 0.8761674 ],\n",
       "       [0.09058579, 0.90941423],\n",
       "       [0.38000017, 0.6199999 ],\n",
       "       [0.1875354 , 0.81246454],\n",
       "       [0.10545559, 0.8945444 ],\n",
       "       [0.4022274 , 0.59777266],\n",
       "       [0.47419956, 0.5258004 ],\n",
       "       [0.4192579 , 0.58074206],\n",
       "       [0.22179022, 0.7782098 ],\n",
       "       [0.48839185, 0.5116081 ],\n",
       "       [0.23859237, 0.7614076 ],\n",
       "       [0.56494176, 0.43505824],\n",
       "       [0.25695306, 0.743047  ],\n",
       "       [0.63054246, 0.3694575 ],\n",
       "       [0.1350218 , 0.8649782 ],\n",
       "       [0.09784724, 0.9021528 ],\n",
       "       [0.44122866, 0.5587714 ],\n",
       "       [0.1413386 , 0.85866135],\n",
       "       [0.41875002, 0.58125   ],\n",
       "       [0.03247627, 0.96752375],\n",
       "       [0.03894655, 0.9610535 ],\n",
       "       [0.06109908, 0.9389009 ],\n",
       "       [0.06035559, 0.93964446],\n",
       "       [0.12252425, 0.8774758 ],\n",
       "       [0.09986112, 0.90013885],\n",
       "       [0.49380815, 0.50619185],\n",
       "       [0.45039353, 0.5496065 ],\n",
       "       [0.06351614, 0.9364839 ],\n",
       "       [0.02655581, 0.9734442 ],\n",
       "       [0.06438145, 0.9356185 ],\n",
       "       [0.0722896 , 0.9277104 ],\n",
       "       [0.1501507 , 0.8498493 ],\n",
       "       [0.5657216 , 0.4342784 ],\n",
       "       [0.3076205 , 0.6923795 ],\n",
       "       [0.70546913, 0.29453087],\n",
       "       [0.02488676, 0.9751133 ],\n",
       "       [0.04011749, 0.9598825 ],\n",
       "       [0.04076347, 0.95923656],\n",
       "       [0.11050492, 0.889495  ],\n",
       "       [0.03581686, 0.9641831 ],\n",
       "       [0.427569  , 0.572431  ],\n",
       "       [0.55794823, 0.44205177],\n",
       "       [0.6437101 , 0.3562899 ],\n",
       "       [0.04928676, 0.9507133 ],\n",
       "       [0.03059803, 0.969402  ],\n",
       "       [0.05110165, 0.9488984 ],\n",
       "       [0.06189087, 0.93810916],\n",
       "       [0.3389682 , 0.6610318 ],\n",
       "       [0.4503145 , 0.54968554],\n",
       "       [0.7236223 , 0.27637765],\n",
       "       [0.46707493, 0.53292507],\n",
       "       [0.2129726 , 0.7870274 ],\n",
       "       [0.2850744 , 0.7149256 ],\n",
       "       [0.17784812, 0.8221519 ],\n",
       "       [0.3588661 , 0.6411339 ],\n",
       "       [0.08427561, 0.9157244 ],\n",
       "       [0.32827914, 0.67172086],\n",
       "       [0.27568382, 0.7243161 ],\n",
       "       [0.42085114, 0.5791488 ],\n",
       "       [0.14349785, 0.8565022 ],\n",
       "       [0.48090544, 0.5190946 ],\n",
       "       [0.13319744, 0.8668025 ],\n",
       "       [0.6232609 , 0.3767391 ],\n",
       "       [0.1377904 , 0.8622096 ],\n",
       "       [0.23051961, 0.76948035],\n",
       "       [0.3047976 , 0.6952024 ],\n",
       "       [0.28733838, 0.7126616 ],\n",
       "       [0.20687209, 0.79312795],\n",
       "       [0.41655087, 0.5834492 ],\n",
       "       [0.29945767, 0.70054233],\n",
       "       [0.24755277, 0.7524472 ],\n",
       "       [0.24353112, 0.75646883],\n",
       "       [0.47184366, 0.52815634],\n",
       "       [0.16871448, 0.83128554],\n",
       "       [0.26119167, 0.73880833],\n",
       "       [0.16684389, 0.8331561 ],\n",
       "       [0.11459456, 0.8854054 ],\n",
       "       [0.24353112, 0.75646883],\n",
       "       [0.50718606, 0.49281397],\n",
       "       [0.12207867, 0.87792134],\n",
       "       [0.40094078, 0.5990592 ],\n",
       "       [0.36715847, 0.6328415 ],\n",
       "       [0.02488676, 0.9751133 ],\n",
       "       [0.0741272 , 0.92587286],\n",
       "       [0.04307704, 0.956923  ],\n",
       "       [0.13677685, 0.86322314],\n",
       "       [0.18382636, 0.8161736 ],\n",
       "       [0.4241795 , 0.5758205 ],\n",
       "       [0.44943365, 0.5505663 ],\n",
       "       [0.6416226 , 0.3583774 ],\n",
       "       [0.03247627, 0.96752375],\n",
       "       [0.03280221, 0.9671978 ],\n",
       "       [0.05674649, 0.9432535 ],\n",
       "       [0.0970318 , 0.9029682 ],\n",
       "       [0.09685463, 0.9031453 ],\n",
       "       [0.52613485, 0.47386518],\n",
       "       [0.29172766, 0.70827234],\n",
       "       [0.5612703 , 0.43872973],\n",
       "       [0.0257523 , 0.9742477 ],\n",
       "       [0.04864973, 0.9513503 ],\n",
       "       [0.04312816, 0.9568718 ],\n",
       "       [0.11677829, 0.8832217 ],\n",
       "       [0.0953956 , 0.9046044 ],\n",
       "       [0.14006004, 0.85994   ],\n",
       "       [0.4634189 , 0.53658116],\n",
       "       [0.5123611 , 0.48763886],\n",
       "       [0.03995551, 0.96004456],\n",
       "       [0.03558156, 0.9644184 ],\n",
       "       [0.10454678, 0.89545316],\n",
       "       [0.06182488, 0.9381751 ],\n",
       "       [0.1774748 , 0.8225252 ],\n",
       "       [0.42981267, 0.5701873 ],\n",
       "       [0.5451214 , 0.45487866],\n",
       "       [0.40936306, 0.59063697],\n",
       "       [0.05361166, 0.9463883 ],\n",
       "       [0.3865851 , 0.61341494],\n",
       "       [0.17617287, 0.8238271 ],\n",
       "       [0.10895917, 0.89104086],\n",
       "       [0.35696128, 0.64303875],\n",
       "       [0.25579438, 0.7442056 ],\n",
       "       [0.27756357, 0.7224364 ],\n",
       "       [0.44764817, 0.55235183],\n",
       "       [0.08584843, 0.9141516 ],\n",
       "       [0.1530247 , 0.8469753 ],\n",
       "       [0.44245037, 0.55754966],\n",
       "       [0.06857848, 0.9314216 ],\n",
       "       [0.21479584, 0.7852042 ],\n",
       "       [0.1957761 , 0.8042239 ],\n",
       "       [0.20768446, 0.7923155 ],\n",
       "       [0.514964  , 0.48503605],\n",
       "       [0.3662889 , 0.6337111 ],\n",
       "       [0.46549574, 0.53450423],\n",
       "       [0.270977  , 0.729023  ],\n",
       "       [0.46022967, 0.5397703 ],\n",
       "       [0.15613204, 0.84386796],\n",
       "       [0.471524  , 0.52847606],\n",
       "       [0.22439516, 0.7756048 ],\n",
       "       [0.12575728, 0.8742427 ],\n",
       "       [0.40188655, 0.5981134 ],\n",
       "       [0.18388075, 0.81611925],\n",
       "       [0.16161403, 0.83838594],\n",
       "       [0.47995734, 0.5200427 ],\n",
       "       [0.1620473 , 0.83795273],\n",
       "       [0.5298098 , 0.4701903 ],\n",
       "       [0.34105897, 0.65894103]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = final_gb.predict(dtest)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 2)\n",
      "(189, 1)\n"
     ]
    }
   ],
   "source": [
    "print(ypred.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TeamA Win% 97.24  SeedDiff 15.0\n",
      "TeamA Win% 97.42  SeedDiff 13.0\n",
      "TeamA Win% 95.48  SeedDiff 11.0\n",
      "TeamA Win% 85.98  SeedDiff 9.0\n",
      "TeamA Win% 86.62  SeedDiff 7.0\n",
      "TeamA Win% 58.53  SeedDiff 5.0\n",
      "TeamA Win% 81.69  SeedDiff 3.0\n",
      "TeamA Win% 23.15  SeedDiff 1.0\n",
      "TeamA Win% 97.51  SeedDiff 15.0\n",
      "TeamA Win% 91.46  SeedDiff 13.0\n",
      "TeamA Win% 94.9  SeedDiff 11.0\n",
      "TeamA Win% 88.95  SeedDiff 9.0\n",
      "TeamA Win% 78.79  SeedDiff 7.0\n",
      "TeamA Win% 72.95  SeedDiff 5.0\n",
      "TeamA Win% 45.39  SeedDiff 3.0\n",
      "TeamA Win% 42.1  SeedDiff 1.0\n",
      "TeamA Win% 97.24  SeedDiff 15.0\n",
      "TeamA Win% 95.9  SeedDiff 13.0\n",
      "TeamA Win% 93.03  SeedDiff 11.0\n",
      "TeamA Win% 94.76  SeedDiff 9.0\n",
      "TeamA Win% 85.38  SeedDiff 7.0\n",
      "TeamA Win% 66.21  SeedDiff 5.0\n",
      "TeamA Win% 60.45  SeedDiff 3.0\n",
      "TeamA Win% 40.49  SeedDiff 1.0\n",
      "TeamA Win% 96.89  SeedDiff 15.0\n",
      "TeamA Win% 88.36  SeedDiff 13.0\n",
      "TeamA Win% 80.23  SeedDiff 11.0\n",
      "TeamA Win% 93.53  SeedDiff 9.0\n",
      "TeamA Win% 78.1  SeedDiff 7.0\n",
      "TeamA Win% 65.04  SeedDiff 5.0\n",
      "TeamA Win% 41.07  SeedDiff 3.0\n",
      "TeamA Win% 51.58  SeedDiff 1.0\n",
      "TeamA Win% 89.95  SeedDiff 8.0\n",
      "TeamA Win% 78.16  SeedDiff 5.0\n",
      "TeamA Win% 65.88  SeedDiff 8.0\n",
      "TeamA Win% 83.36  SeedDiff 8.0\n",
      "TeamA Win% 83.82  SeedDiff 7.0\n",
      "TeamA Win% 77.4  SeedDiff 8.0\n",
      "TeamA Win% 85.32  SeedDiff 8.0\n",
      "TeamA Win% 90.69  SeedDiff 8.0\n",
      "TeamA Win% 88.07  SeedDiff 8.0\n",
      "TeamA Win% 58.81  SeedDiff 5.0\n",
      "TeamA Win% 64.86  SeedDiff 8.0\n",
      "TeamA Win% 68.33  SeedDiff 1.0\n",
      "TeamA Win% 87.62  SeedDiff 8.0\n",
      "TeamA Win% 90.94  SeedDiff 5.0\n",
      "TeamA Win% 62.0  SeedDiff 8.0\n",
      "TeamA Win% 81.25  SeedDiff 8.0\n",
      "TeamA Win% 89.45  SeedDiff 4.0\n",
      "TeamA Win% 59.78  SeedDiff 1.0\n",
      "TeamA Win% 52.58  SeedDiff 3.0\n",
      "TeamA Win% 58.07  SeedDiff 1.0\n",
      "TeamA Win% 77.82  SeedDiff 4.0\n",
      "TeamA Win% 51.16  SeedDiff 1.0\n",
      "TeamA Win% 76.14  SeedDiff 3.0\n",
      "TeamA Win% 43.51  SeedDiff 1.0\n",
      "TeamA Win% 74.3  SeedDiff 1.0\n",
      "TeamA Win% 36.95  SeedDiff 1.0\n",
      "TeamA Win% 86.5  SeedDiff 5.0\n",
      "TeamA Win% 90.22  SeedDiff 9.0\n",
      "TeamA Win% 55.88  SeedDiff -0.0\n",
      "TeamA Win% 85.87  SeedDiff 9.0\n",
      "TeamA Win% 58.13  SeedDiff 1.0\n",
      "TeamA Win% 96.75  SeedDiff 15.0\n",
      "TeamA Win% 96.11  SeedDiff 13.0\n",
      "TeamA Win% 93.89  SeedDiff 11.0\n",
      "TeamA Win% 93.96  SeedDiff 9.0\n",
      "TeamA Win% 87.75  SeedDiff 7.0\n",
      "TeamA Win% 90.01  SeedDiff 5.0\n",
      "TeamA Win% 50.62  SeedDiff 3.0\n",
      "TeamA Win% 54.96  SeedDiff 1.0\n",
      "TeamA Win% 93.65  SeedDiff 15.0\n",
      "TeamA Win% 97.34  SeedDiff 13.0\n",
      "TeamA Win% 93.56  SeedDiff 11.0\n",
      "TeamA Win% 92.77  SeedDiff 9.0\n",
      "TeamA Win% 84.98  SeedDiff 7.0\n",
      "TeamA Win% 43.43  SeedDiff 5.0\n",
      "TeamA Win% 69.24  SeedDiff 3.0\n",
      "TeamA Win% 29.45  SeedDiff 1.0\n",
      "TeamA Win% 97.51  SeedDiff 15.0\n",
      "TeamA Win% 95.99  SeedDiff 13.0\n",
      "TeamA Win% 95.92  SeedDiff 11.0\n",
      "TeamA Win% 88.95  SeedDiff 9.0\n",
      "TeamA Win% 96.42  SeedDiff 7.0\n",
      "TeamA Win% 57.24  SeedDiff 5.0\n",
      "TeamA Win% 44.21  SeedDiff 3.0\n",
      "TeamA Win% 35.63  SeedDiff 1.0\n",
      "TeamA Win% 95.07  SeedDiff 15.0\n",
      "TeamA Win% 96.94  SeedDiff 13.0\n",
      "TeamA Win% 94.89  SeedDiff 11.0\n",
      "TeamA Win% 93.81  SeedDiff 9.0\n",
      "TeamA Win% 66.1  SeedDiff 7.0\n",
      "TeamA Win% 54.97  SeedDiff 5.0\n",
      "TeamA Win% 27.64  SeedDiff 3.0\n",
      "TeamA Win% 53.29  SeedDiff 1.0\n",
      "TeamA Win% 78.7  SeedDiff 7.0\n",
      "TeamA Win% 71.49  SeedDiff 5.0\n",
      "TeamA Win% 82.22  SeedDiff 8.0\n",
      "TeamA Win% 64.11  SeedDiff 1.0\n",
      "TeamA Win% 91.57  SeedDiff 7.0\n",
      "TeamA Win% 67.17  SeedDiff 5.0\n",
      "TeamA Win% 72.43  SeedDiff 8.0\n",
      "TeamA Win% 57.91  SeedDiff 1.0\n",
      "TeamA Win% 85.65  SeedDiff 8.0\n",
      "TeamA Win% 51.91  SeedDiff 5.0\n",
      "TeamA Win% 86.68  SeedDiff 8.0\n",
      "TeamA Win% 37.67  SeedDiff 1.0\n",
      "TeamA Win% 86.22  SeedDiff 7.0\n",
      "TeamA Win% 76.95  SeedDiff 8.0\n",
      "TeamA Win% 69.52  SeedDiff 3.0\n",
      "TeamA Win% 71.27  SeedDiff 8.0\n",
      "TeamA Win% 79.31  SeedDiff 4.0\n",
      "TeamA Win% 58.34  SeedDiff 4.0\n",
      "TeamA Win% 70.05  SeedDiff 3.0\n",
      "TeamA Win% 75.24  SeedDiff 9.0\n",
      "TeamA Win% 75.65  SeedDiff 3.0\n",
      "TeamA Win% 52.82  SeedDiff 4.0\n",
      "TeamA Win% 83.13  SeedDiff 3.0\n",
      "TeamA Win% 73.88  SeedDiff 1.0\n",
      "TeamA Win% 83.32  SeedDiff 3.0\n",
      "TeamA Win% 88.54  SeedDiff 10.0\n",
      "TeamA Win% 75.65  SeedDiff 2.0\n",
      "TeamA Win% 49.28  SeedDiff 1.0\n",
      "TeamA Win% 87.79  SeedDiff 6.0\n",
      "TeamA Win% 59.91  SeedDiff 2.0\n",
      "TeamA Win% 63.28  SeedDiff -0.0\n",
      "TeamA Win% 97.51  SeedDiff 15.0\n",
      "TeamA Win% 92.59  SeedDiff 13.0\n",
      "TeamA Win% 95.69  SeedDiff 11.0\n",
      "TeamA Win% 86.32  SeedDiff 9.0\n",
      "TeamA Win% 81.62  SeedDiff 7.0\n",
      "TeamA Win% 57.58  SeedDiff 5.0\n",
      "TeamA Win% 55.06  SeedDiff 3.0\n",
      "TeamA Win% 35.84  SeedDiff 1.0\n",
      "TeamA Win% 96.75  SeedDiff 15.0\n",
      "TeamA Win% 96.72  SeedDiff 13.0\n",
      "TeamA Win% 94.33  SeedDiff 11.0\n",
      "TeamA Win% 90.3  SeedDiff 9.0\n",
      "TeamA Win% 90.31  SeedDiff 7.0\n",
      "TeamA Win% 47.39  SeedDiff 5.0\n",
      "TeamA Win% 70.83  SeedDiff 3.0\n",
      "TeamA Win% 43.87  SeedDiff 1.0\n",
      "TeamA Win% 97.42  SeedDiff 15.0\n",
      "TeamA Win% 95.14  SeedDiff 13.0\n",
      "TeamA Win% 95.69  SeedDiff 11.0\n",
      "TeamA Win% 88.32  SeedDiff 9.0\n",
      "TeamA Win% 90.46  SeedDiff 7.0\n",
      "TeamA Win% 85.99  SeedDiff 5.0\n",
      "TeamA Win% 53.66  SeedDiff 3.0\n",
      "TeamA Win% 48.76  SeedDiff 1.0\n",
      "TeamA Win% 96.0  SeedDiff 15.0\n",
      "TeamA Win% 96.44  SeedDiff 13.0\n",
      "TeamA Win% 89.55  SeedDiff 11.0\n",
      "TeamA Win% 93.82  SeedDiff 9.0\n",
      "TeamA Win% 82.25  SeedDiff 7.0\n",
      "TeamA Win% 57.02  SeedDiff 5.0\n",
      "TeamA Win% 45.49  SeedDiff 3.0\n",
      "TeamA Win% 59.06  SeedDiff 1.0\n",
      "TeamA Win% 94.64  SeedDiff 7.0\n",
      "TeamA Win% 61.34  SeedDiff 5.0\n",
      "TeamA Win% 82.38  SeedDiff 8.0\n",
      "TeamA Win% 89.1  SeedDiff 8.0\n",
      "TeamA Win% 64.3  SeedDiff 8.0\n",
      "TeamA Win% 74.42  SeedDiff 5.0\n",
      "TeamA Win% 72.24  SeedDiff 3.0\n",
      "TeamA Win% 55.24  SeedDiff 1.0\n",
      "TeamA Win% 91.42  SeedDiff 8.0\n",
      "TeamA Win% 84.7  SeedDiff 8.0\n",
      "TeamA Win% 55.75  SeedDiff 3.0\n",
      "TeamA Win% 93.14  SeedDiff 8.0\n",
      "TeamA Win% 78.52  SeedDiff 7.0\n",
      "TeamA Win% 80.42  SeedDiff 5.0\n",
      "TeamA Win% 79.23  SeedDiff 8.0\n",
      "TeamA Win% 48.5  SeedDiff 1.0\n",
      "TeamA Win% 63.37  SeedDiff 4.0\n",
      "TeamA Win% 53.45  SeedDiff 4.0\n",
      "TeamA Win% 72.9  SeedDiff 5.0\n",
      "TeamA Win% 53.98  SeedDiff 4.0\n",
      "TeamA Win% 84.39  SeedDiff 4.0\n",
      "TeamA Win% 52.85  SeedDiff 1.0\n",
      "TeamA Win% 77.56  SeedDiff 4.0\n",
      "TeamA Win% 87.42  SeedDiff 9.0\n",
      "TeamA Win% 59.81  SeedDiff 2.0\n",
      "TeamA Win% 81.61  SeedDiff 6.0\n",
      "TeamA Win% 83.84  SeedDiff 2.0\n",
      "TeamA Win% 52.0  SeedDiff 1.0\n",
      "TeamA Win% 83.8  SeedDiff 8.0\n",
      "TeamA Win% 47.02  SeedDiff -0.0\n",
      "TeamA Win% 65.89  SeedDiff 2.0\n"
     ]
    }
   ],
   "source": [
    "correct_list = np.zeros_like(testing_labels)\n",
    "for i in range(len(ypred)):\n",
    "    if ypred[i][0] < ypred[i][1]:\n",
    "        metric = 1 # team A predicted to win\n",
    "    else:\n",
    "        metric = 0\n",
    "    correct_list[i][0] = metric\n",
    "    print(\"TeamA Win% {:.4}  SeedDiff {}\".format(ypred[i][1]*100,testing_data[i][4]*-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Correct:  88.88888888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"% Correct: \", (correct_list.sum() / len(correct_list))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
